{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import imageio\n",
    "# import litellm\n",
    "\n",
    "# Grounding DINO\n",
    "# import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "# from GroundingDINO.groundingdino.models import build_model\n",
    "# from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "# from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "\n",
    "# # segment anything\n",
    "# from segment_anything import (\n",
    "#     build_sam,\n",
    "#     build_sam_hq,\n",
    "#     SamPredictor\n",
    "# ) \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Recognize Anything Model & Tag2Text\n",
    "# from ram.models import ram\n",
    "# from ram import inference_ram\n",
    "# import torchvision.transforms as TS\n",
    "\n",
    "from functools import partial\n",
    "from PCR_CG.lib.timer import Timer\n",
    "from PCR_CG.lib.utils import load_obj, natural_key\n",
    "from PCR_CG.datasets.indoor import IndoorDataset\n",
    "from PCR_CG.datasets.modelnet import get_train_datasets, get_test_datasets\n",
    "import os,re,sys,json,yaml,random, argparse, torch, pickle\n",
    "from easydict import EasyDict as edict\n",
    "from PCR_CG.configs.models import architectures\n",
    "from PCR_CG.models.architectures import KPFCNN\n",
    "import open3d as o3d\n",
    "# import open3d.visualization.jupyter as o3d_jupyter\n",
    "import plotly.graph_objects as go\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_config(path):\n",
    "    \"\"\"\n",
    "    Loads config file:\n",
    "\n",
    "    Args:\n",
    "        path (str): path to the config file\n",
    "\n",
    "    Returns: \n",
    "        config (dict): dictionary of the configuration parameters, merge sub_dicts\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path,'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    \n",
    "    config = dict()\n",
    "    for key, value in cfg.items():\n",
    "        for k,v in value.items():\n",
    "            config[k] = v\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_datasets(config):\n",
    "    info_train = load_obj(config.train_info)\n",
    "    # print(\"=======\",info_train)\n",
    "    train_set = IndoorDataset(info_train,config,data_augmentation=True)\n",
    "    return train_set\n",
    "\n",
    "# ChatGPT or nltk is required when using tags_chineses\n",
    "# import openai\n",
    "# import nltk\n",
    "\n",
    "def load_image(image_path):\n",
    "    # load image\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")  # load image\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image, _ = transform(image_pil, None)  # 3, h, w\n",
    "    return image_pil, image\n",
    "\n",
    "\n",
    "def check_tags_chinese(tags_chinese, pred_phrases, max_tokens=100, model=\"gpt-3.5-turbo\"):\n",
    "    object_list = [obj.split('(')[0] for obj in pred_phrases]\n",
    "    object_num = []\n",
    "    for obj in set(object_list):\n",
    "        object_num.append(f'{object_list.count(obj)} {obj}')\n",
    "    object_num = ', '.join(object_num)\n",
    "    print(f\"Correct object number: {object_num}\")\n",
    "\n",
    "    if openai_key:\n",
    "        prompt = [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'Revise the number in the tags_chinese if it is wrong. ' + \\\n",
    "                           f'tags_chinese: {tags_chinese}. ' + \\\n",
    "                           f'True object number: {object_num}. ' + \\\n",
    "                           'Only give the revised tags_chinese: '\n",
    "            }\n",
    "        ]\n",
    "        response = litellm.completion(model=model, messages=prompt, temperature=0.6, max_tokens=max_tokens)\n",
    "        reply = response['choices'][0]['message']['content']\n",
    "        # sometimes return with \"tags_chinese: xxx, xxx, xxx\"\n",
    "        tags_chinese = reply.split(':')[-1].strip()\n",
    "    return tags_chinese\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class PointCloudToImageMapper(object):\n",
    "    def __init__(\n",
    "            self, image_dim, visibility_threshold=0.1, cut_bound=0, intrinsics=None, device=\"cpu\",\n",
    "            use_torch=False, eps=1e-8):\n",
    "\n",
    "        self.image_dim = image_dim\n",
    "        self.vis_thres = visibility_threshold\n",
    "        self.cut_bound = cut_bound\n",
    "        self.intrinsics = intrinsics\n",
    "        self.eps = eps\n",
    "\n",
    "        self.device = device\n",
    "        if use_torch:\n",
    "            self.intrinsics = torch.from_numpy(self.intrinsics).to(device)\n",
    "\n",
    "    def compute_mapping_torch(self, camera_to_world, coords, depth=None, intrinsic=None, vis_thresh=None):\n",
    "        \"\"\"\n",
    "        :param camera_to_world: 4 x 4\n",
    "        :param coords: N x 3 format\n",
    "        :param depth: H x W format\n",
    "        :param intrinsic: 3x3 format\n",
    "        :return: mapping, N x 3 format, (H,W,mask)\n",
    "        \"\"\"\n",
    "\n",
    "        depth = depth.squeeze(0)\n",
    "        print(depth.shape)\n",
    "        device = coords.device\n",
    "        if vis_thresh is not None:\n",
    "            self.vis_thres = vis_thresh\n",
    "        if intrinsic is not None:  # adjust intrinsic\n",
    "            self.intrinsics = intrinsic\n",
    "        else:\n",
    "            intrinsic = self.intrinsics\n",
    "    \n",
    "        camera_to_world = torch.from_numpy(camera_to_world).float()\n",
    "        # camera_to_world = camera_to_world.to(device).float()\n",
    "        mapping = torch.zeros((3, coords.shape[0]), dtype=torch.long, device=device)\n",
    "        coords_new = torch.cat([coords, torch.ones([coords.shape[0], 1], dtype=torch.float, device=device)], dim=1).T\n",
    "\n",
    "        assert coords_new.shape[0] == 4, \"[!] Shape error\"\n",
    "\n",
    "        world_to_camera = torch.linalg.inv(camera_to_world)\n",
    "        p = world_to_camera.float() @ coords_new.float()\n",
    "        print(p.shape)\n",
    "        # p =  torch.from_numpy(self.intrinsics) @ p\n",
    "        # p[2][torch.abs(p[2]) < self.eps] = self.eps\n",
    "        p[0] = (p[0] * intrinsic[0][0]) / p[2] + intrinsic[0][2]\n",
    "        p[1] = (p[1] * intrinsic[1][1]) / p[2] + intrinsic[1][2]\n",
    "        pi = torch.round(p).long()  # simply round the projected coordinates\n",
    "        inside_mask = (\n",
    "                (pi[0] >= self.cut_bound)\n",
    "                * (pi[1] >= self.cut_bound)\n",
    "                * (pi[0] < self.image_dim[0] - self.cut_bound)\n",
    "                * (pi[1] < self.image_dim[1] - self.cut_bound)\n",
    "        )\n",
    "        if depth is not None:\n",
    "            depth = torch.from_numpy(depth).to(device)\n",
    "            # print(inside_mask.shape, depth.shape, pi.shape, pi[1][inside_mask].max(), pi[0][inside_mask].max())\n",
    "            occlusion_mask = torch.abs(\n",
    "                depth[pi[1][inside_mask], pi[0][inside_mask]] - p[2][inside_mask]) <= self.vis_thres\n",
    "            inside_mask[inside_mask == True] = occlusion_mask.clone()\n",
    "        else:\n",
    "            front_mask = p[2] > 0  # make sure the depth is in front\n",
    "            inside_mask = front_mask * inside_mask\n",
    "\n",
    "        new_inside_mask = inside_mask\n",
    "\n",
    "        mapping[0][new_inside_mask] = pi[1][new_inside_mask]\n",
    "        mapping[1][new_inside_mask] = pi[0][new_inside_mask]\n",
    "        mapping[2][new_inside_mask] = 1\n",
    "\n",
    "        return mapping.T\n",
    "\n",
    "    def compute_mapping(self, camera_to_world, coords, depth=None, intrinsic=None):\n",
    "        \"\"\"\n",
    "        :param camera_to_world: 4 x 4\n",
    "        :param coords: N x 3 format\n",
    "        :param depth: H x W format\n",
    "        :param intrinsic: 3x3 format\n",
    "        :return: mapping, N x 3 format, (H,W,mask)\n",
    "        \"\"\"\n",
    "        if intrinsic is not None:  # adjust intrinsic\n",
    "            self.intrinsics = intrinsic\n",
    "        else:\n",
    "            intrinsic = self.intrinsics\n",
    "\n",
    "        mapping = np.zeros((3, coords.shape[0]), dtype=int)\n",
    "        coords_new = np.concatenate([coords, np.ones([coords.shape[0], 1])], axis=1).T\n",
    "        assert coords_new.shape[0] == 4, \"[!] Shape error\"\n",
    "\n",
    "        world_to_camera = np.linalg.inv(camera_to_world)\n",
    "        p = np.matmul(world_to_camera, coords_new)\n",
    "        p[2][np.abs(p[2]) < self.eps] = self.eps\n",
    "        p[0] = (p[0] * intrinsic[0][0]) / p[2] + intrinsic[0][2]\n",
    "        p[1] = (p[1] * intrinsic[1][1]) / p[2] + intrinsic[1][2]\n",
    "        pi = np.round(p).astype(np.int32)  # simply round the projected coordinates\n",
    "        inside_mask = (\n",
    "                (pi[0] >= self.cut_bound)\n",
    "                * (pi[1] >= self.cut_bound)\n",
    "                * (pi[0] < self.image_dim[0] - self.cut_bound)\n",
    "                * (pi[1] < self.image_dim[1] - self.cut_bound)\n",
    "        )\n",
    "        if depth is not None:\n",
    "            depth_cur = depth[pi[1][inside_mask], pi[0][inside_mask]]\n",
    "            occlusion_mask = (\n",
    "                    np.abs(\n",
    "                        depth[pi[1][inside_mask], pi[0][inside_mask]] - p[2][inside_mask]) <= self.vis_thres * depth_cur\n",
    "            )\n",
    "            inside_mask[inside_mask == True] = occlusion_mask\n",
    "        else:\n",
    "            front_mask = p[2] > 0  # make sure the depth is in front\n",
    "            inside_mask = front_mask * inside_mask\n",
    "\n",
    "        # NOTE detect occlusion\n",
    "        pi_x_ = pi[1][inside_mask]\n",
    "        pi_y_ = pi[0][inside_mask]\n",
    "        pi_depth_ = pi[2][inside_mask]\n",
    "\n",
    "        inds = (pi_x_ * self.image_dim[0] + pi_y_).astype(np.int32)\n",
    "        _, inds = np.unique(inds, return_inverse=True)\n",
    "\n",
    "        depth_min = torch_scatter.scatter_min(\n",
    "            torch.from_numpy(pi_depth_).float(), torch.from_numpy(inds).long(), dim=0\n",
    "        )[0]\n",
    "        depth_min = torch.where(depth_min < 0.0, 0.0, depth_min)\n",
    "        depth_min = depth_min.numpy()\n",
    "        depth_min_broadcast = depth_min[inds]\n",
    "\n",
    "        THRESHOLD = 0.2  # (meter)\n",
    "        depth_occlusion_mask = (pi_depth_ - depth_min_broadcast) <= THRESHOLD\n",
    "\n",
    "        new_inside_mask = inside_mask.copy()\n",
    "        new_inside_mask[inside_mask] = depth_occlusion_mask\n",
    "        ############################\n",
    "\n",
    "        mapping[0][new_inside_mask] = pi[1][new_inside_mask]\n",
    "        mapping[1][new_inside_mask] = pi[0][new_inside_mask]\n",
    "        mapping[2][new_inside_mask] = 1\n",
    "\n",
    "        return mapping.T\n",
    "\n",
    "def adjust_intrinsic(intrinsic, intrinsic_image_dim, image_dim):\n",
    "\n",
    "    if intrinsic_image_dim == image_dim:\n",
    "        return intrinsic\n",
    "\n",
    "    intrinsic_return = np.copy(intrinsic)\n",
    "\n",
    "    height_after = image_dim[1]\n",
    "    height_before = intrinsic_image_dim[1]\n",
    "    height_ratio = height_after / height_before\n",
    "\n",
    "    width_after = image_dim[0]\n",
    "    width_before = intrinsic_image_dim[0]\n",
    "    width_ratio = width_after / width_before\n",
    "\n",
    "    if width_ratio >= height_ratio:\n",
    "        resize_height = height_after\n",
    "        resize_width = height_ratio * width_before\n",
    "\n",
    "    else:\n",
    "        resize_width = width_after\n",
    "        resize_height = width_ratio * height_before\n",
    "\n",
    "    intrinsic_return[0,0] *= float(resize_width)/float(width_before)\n",
    "    intrinsic_return[1,1] *= float(resize_height)/float(height_before)\n",
    "    # account for cropping/padding here\n",
    "    intrinsic_return[0,2] *= float(resize_width-1)/float(width_before-1)\n",
    "    intrinsic_return[1,2] *= float(resize_height-1)/float(height_before-1)\n",
    "\n",
    "\n",
    "\n",
    "    return intrinsic_return    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    config = edict(load_config('./PCR_CG/configs/train/indoor.yaml'))\n",
    "    # print(config)\n",
    "    train_set = get_datasets(config)\n",
    "    print(train_set)\n",
    "    # print(train_set[500])\n",
    "    point = train_set[10]\n",
    "    # torch.save(point,'/home/cx/cv_project/Open3DIS/pointdata.pth')\n",
    "    print('已保存')\n",
    "    src_path = point['src_path']\n",
    "    tgt_path = point['tgt_path']\n",
    "    parser = argparse.ArgumentParser(\"Grounded-Segment-Anything Demo\", add_help=True)\n",
    "    parser.add_argument(\"--config\", type=str, required=False, help=\"path to config file\")\n",
    "    parser.add_argument(\n",
    "        \"--ram_checkpoint\", type=str, required=False, help=\"path to checkpoint file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--grounded_checkpoint\", type=str, required=False, help=\"path to checkpoint file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sam_checkpoint\", type=str, required=False, help=\"path to checkpoint file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sam_hq_checkpoint\", type=str, default=None, help=\"path to sam-hq checkpoint file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_sam_hq\", action=\"store_true\", help=\"using sam-hq for prediction\"\n",
    "    )\n",
    "    parser.add_argument(\"--input_image\", type=str, required=False, help=\"path to image file\")\n",
    "    parser.add_argument(\"--split\", default=\",\", type=str, help=\"split for text prompt\")\n",
    "    parser.add_argument(\"--openai_key\", type=str, help=\"key for chatgpt\")\n",
    "    parser.add_argument(\"--openai_proxy\", default=None, type=str, help=\"proxy for chatgpt\")\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", \"-o\", type=str, default=\"outputs\", required=False, help=\"output directory\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--box_threshold\", type=float, default=0.25, help=\"box threshold\")\n",
    "    parser.add_argument(\"--text_threshold\", type=float, default=0.2, help=\"text threshold\")\n",
    "    parser.add_argument(\"--iou_threshold\", type=float, default=0.5, help=\"iou threshold\")\n",
    "\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cpu\", help=\"running on cpu only!, default=False\")\n",
    "    args = parser.parse_known_args()\n",
    "\n",
    "    # cfg\n",
    "    config_file = 'GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'  # change the path of the model config file\n",
    "    ram_checkpoint = './ram_swin_large_14m.pth'  # change the path of the model\n",
    "    grounded_checkpoint = './groundingdino_swint_ogc.pth'  # change the path of the model\n",
    "    sam_checkpoint = './sam_vit_h_4b8939.pth'\n",
    "    sam_hq_checkpoint = None\n",
    "    use_sam_hq = ''\n",
    "    image_path_src = src_path\n",
    "    image_path_tgt = tgt_path\n",
    "    split = ','\n",
    "    openai_key = ''\n",
    "    openai_proxy = None\n",
    "    output_dir = \"outputs_src\"\n",
    "    output_dir_tgt = \"outputs_tgt\"\n",
    "    box_threshold = 0.25\n",
    "    text_threshold = 0.2\n",
    "    iou_threshold = 0.5\n",
    "    device = \"cpu\"\n",
    "    depth_scale = 1000.0\n",
    "    img_dim = (640, 480)\n",
    "    cut_num_pixel_boundary = 10\n",
    "\n",
    "\n",
    "    # make dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # load 3D data (point cloud)\n",
    "    points =  point['src_pcd']\n",
    "    points = torch.from_numpy(points).to(device)\n",
    "    n_points = points.shape[0]\n",
    "\n",
    "    n_finished = 0\n",
    "\n",
    "    # short hand for processing 2D features\n",
    "    img_dir = point['src_path']\n",
    "    # num_img = len(img_dirs)\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    n_points_cur = n_points\n",
    "    counter = torch.zeros((n_points_cur, 1), device=device)\n",
    "    mapping = torch.ones([n_points, 4], dtype=int, device=device)\n",
    "\n",
    "    ################ Feature Fusion ###################\n",
    "    # load pose\n",
    "    posepath = img_dir.replace('color', 'pose').replace('.png', '.txt')\n",
    "    pose = np.loadtxt(posepath).astype(float)\n",
    "\n",
    "    # load depth and convert to meter\n",
    "    depth = point['src_depth_image']\n",
    "    depth = depth.cpu().numpy()\n",
    "    origin_intrinsics = point['origin_intrinsics']\n",
    "    big_size, image_size = [640, 480], [640, 480]\n",
    "    # tranpose height and width\n",
    "    intrinsics = adjust_intrinsic(origin_intrinsics, big_size, image_size)\n",
    "    print('intrinsics',intrinsics)\n",
    "    pose = np.loadtxt(posepath).astype(float)\n",
    "    print('pose',pose)\n",
    "    #                     尝试过indoor.py带回来的world2camera数据\n",
    "    src1_world2camera = point['src1_world2camera']\n",
    "    # camera2world = point['camera2world']\n",
    "    # color_name = os.path.basename(img_dir).split('.')[0]\n",
    "    # masks = torch.load(join(data_root_mask, 'maskraw_{}.pth'.format(color_name)))['mask']\n",
    "    ### Cannot query directly 200 classes so split them into multiple chunks -- see Supplementary\n",
    "    image = Image.open(point['src_path']).convert(\"RGB\").resize(img_dim)\n",
    "    image = np.array(image)\n",
    "    pointcloud_mapper = PointCloudToImageMapper(\n",
    "            image_dim=img_dim, intrinsics=intrinsics, cut_bound=cut_num_pixel_boundary\n",
    "        )\n",
    "    #                     这里传的是pose.txt文件里的camera2world数据\n",
    "    mapping[:, 1:4] = pointcloud_mapper.compute_mapping_torch(\n",
    "        pose, points, depth)\n",
    "    # new_mapping = scaling_mapping(\n",
    "    #     torch.squeeze(mapping[:, 1:3]), img_dim[1], img_dim[0], rgb_img_dim[0], rgb_img_dim[1]\n",
    "    # )\n",
    "    # mapping[:, 1:4] = torch.cat((new_mapping, mapping[:, 3].unsqueeze(1)), dim=1)\n",
    "    # valid_mask = mapping[:, 2] == 1\n",
    "    # valid_points = points[valid_mask]\n",
    "    # 为点云添加颜色\n",
    "    h, w = mapping[:, 0], mapping[:, 1]\n",
    "    colors = image[h, w] / 255 # 归一化\n",
    "    print(colors)\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Scatter3d(\n",
    "            x=points[:,0], y=points[:,1], z=points[:,2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=1, color=colors)\n",
    "    )\n",
    "    ],\n",
    "    layout=dict(\n",
    "        scene=dict(\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        zaxis=dict(visible=False)\n",
    "    )\n",
    "    )\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "      # 创建 Open3D 点云对象\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    # 设置点云坐标\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "    # 设置颜色\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    # 保存为 PLY 文件\n",
    "    o3d.io.write_point_cloud('src_mapping.ply', pcd)\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
